{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa32372d",
   "metadata": {},
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe5c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6676e",
   "metadata": {},
   "source": [
    "#Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c4b72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37778\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(r\"C:\\Datasets\\VeRi\\VeRi\")\n",
    "NUM_WORKERS = 0\n",
    "EMBED_DIM = 512\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(len(list((DATA_ROOT/\"image_train\").glob(\"*.jpg\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98979268",
   "metadata": {},
   "source": [
    "#Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd208c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(qf, gf):\n",
    "    # cosine distance = 1 - cosine similarity\n",
    "    sim = F.linear(qf, gf)     # (num_query, num_gallery)\n",
    "    dist = 1 - sim\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2720826",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(model, loader):\n",
    "    model.eval()\n",
    "    feats = []\n",
    "    pids = []\n",
    "    camids = []\n",
    "\n",
    "    for imgs, _, p, c in tqdm(loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        f, _ = model(imgs)     # normalized embeddings (L2 norm)\n",
    "        feats.append(f.cpu())\n",
    "        pids.extend(p)\n",
    "        camids.extend(c)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    pids = torch.tensor(pids)\n",
    "    camids = torch.tensor(camids)\n",
    "\n",
    "    return feats, pids, camids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c55acf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchHardTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Batch-hard triplet loss from:\n",
    "    'In Defense of the Triplet Loss for Person Re-Identification'\n",
    "\n",
    "    Expects:\n",
    "      embeddings: (B, D) L2-normalized\n",
    "      labels: (B,) long\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        # embeddings: [B, D], labels: [B]\n",
    "        dist_mat = torch.cdist(embeddings, embeddings, p=2)  # [B, B]\n",
    "\n",
    "        # same-class mask & different-class mask\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask_pos = labels.eq(labels.t())        # [B, B]\n",
    "        mask_neg = ~mask_pos\n",
    "\n",
    "        # hardest positive for each anchor\n",
    "        dist_pos = dist_mat.clone()\n",
    "        dist_pos[~mask_pos] = -1.0              # ignore non-positives\n",
    "        hardest_pos, _ = dist_pos.max(dim=1)    # [B]\n",
    "\n",
    "        # hardest negative for each anchor\n",
    "        dist_neg = dist_mat.clone()\n",
    "        dist_neg[~mask_neg] = 1e9               # ignore non-negatives\n",
    "        hardest_neg, _ = dist_neg.min(dim=1)    # [B]\n",
    "\n",
    "        # triplet loss\n",
    "        losses = F.relu(hardest_pos - hardest_neg + self.margin)\n",
    "        return losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9891ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, epoch=1,\n",
    "                    use_triplet=True, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    alpha: weight for CE loss\n",
    "    beta:  weight for Triplet loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_ce, total_tri = 0.0, 0.0\n",
    "\n",
    "    for i, (imgs, labels, _, _) in enumerate(loader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        feats, logits = model(imgs)\n",
    "\n",
    "        loss_ce = criterion_ce(logits, labels)\n",
    "        if use_triplet:\n",
    "            loss_tri = criterion_tri(feats, labels)\n",
    "        else:\n",
    "            loss_tri = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "        loss = alpha * loss_ce + beta * loss_tri\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = imgs.size(0)\n",
    "        total_ce += loss_ce.item() * bs\n",
    "        total_tri += loss_tri.item() * bs\n",
    "\n",
    "        # Optional: progress print every 100 batches\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, batch {i+1}/{len(loader)}, \"\n",
    "                  f\"CE={loss_ce.item():.4f}, TRI={loss_tri.item():.4f}\")\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_ce / n, total_tri / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98637107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(distmat, q_pids, g_pids, q_camids, g_camids):\n",
    "    num_q, num_g = distmat.size()\n",
    "    indices = torch.argsort(distmat, dim=1)  # sort ascending (smaller dist = better)\n",
    "\n",
    "    matches = (g_pids[indices] == q_pids[:, None]).float()\n",
    "\n",
    "    all_AP = []\n",
    "    all_rank1 = []\n",
    "\n",
    "    for i in range(num_q):\n",
    "        # Remove gallery samples from the same camera & same PID\n",
    "        valid = ~((g_pids == q_pids[i]) & (g_camids == q_camids[i]))\n",
    "        y_true = matches[i][valid[indices[i]]]\n",
    "        \n",
    "        if y_true.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Compute cumulative precision\n",
    "        y_cum = torch.cumsum(y_true, dim=0)\n",
    "        precision = y_cum / torch.arange(1, len(y_cum)+1).float()\n",
    "\n",
    "        AP = (precision * y_true).sum() / y_true.sum()\n",
    "        all_AP.append(AP.item())\n",
    "\n",
    "        # Rank-1 accuracy\n",
    "        all_rank1.append(y_true[0].item())\n",
    "\n",
    "    mAP = sum(all_AP) / len(all_AP)\n",
    "    rank1 = sum(all_rank1) / len(all_rank1)\n",
    "\n",
    "    return mAP, rank1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ffcd0",
   "metadata": {},
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96154119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeRiDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.paths = sorted(list(self.img_dir.glob(\"*.jpg\")))\n",
    "        self.pids = []\n",
    "        self.camids = []\n",
    "\n",
    "        for p in self.paths:\n",
    "            fname = p.stem\n",
    "            pid_str, cam_str, *_ = fname.split(\"_\")\n",
    "            self.pids.append(int(pid_str))\n",
    "            self.camids.append(int(cam_str[1:]))\n",
    "\n",
    "        unique_pids = sorted(set(self.pids))\n",
    "        self.pid2label = {p: i for i, p in enumerate(unique_pids)}\n",
    "        self.labels = [self.pid2label[p] for p in self.pids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.labels[idx], self.pids[idx], self.camids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609d3958",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_tri = BatchHardTripletLoss(margin=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e433781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_set = VeRiDataset(DATA_ROOT / \"image_train\", transform=train_tf)\n",
    "query_set = VeRiDataset(DATA_ROOT / \"image_query\", transform=test_tf)\n",
    "gallery_set = VeRiDataset(DATA_ROOT / \"image_test\", transform=test_tf)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True,num_workers=NUM_WORKERS,pin_memory=True)\n",
    "query_loader = DataLoader(query_set, batch_size=32, shuffle=False,num_workers=NUM_WORKERS,pin_memory=True)\n",
    "gallery_loader = DataLoader(gallery_set, batch_size=32, shuffle=False,num_workers=NUM_WORKERS,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d569fde2",
   "metadata": {},
   "source": [
    "#Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bead297",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_set.pid2label)\n",
    "class ReIDNet(nn.Module):\n",
    "    def __init__(self, arch=\"resnet50\", num_classes=NUM_CLASSES, embed_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        if arch == \"resnet50\":\n",
    "            base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            base = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # keep everything except the final FC\n",
    "        self.features = nn.Sequential(*list(base.children())[:-1])  # up to avgpool\n",
    "        in_features = base.fc.in_features\n",
    "\n",
    "        # embedding head + classifier\n",
    "        self.embed = nn.Linear(in_features, embed_dim)\n",
    "        self.bn = nn.BatchNorm1d(embed_dim)\n",
    "        self.bn.bias.requires_grad_(False)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)              # [B, C, 1, 1]\n",
    "        x = x.view(x.size(0), -1)         # [B, C]\n",
    "        feat = self.embed(x)              # [B, D]\n",
    "        feat = self.bn(feat)\n",
    "        feat_norm = F.normalize(feat, p=2, dim=1)\n",
    "        logits = self.classifier(feat)\n",
    "        return feat_norm, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87897d6d",
   "metadata": {},
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23542835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 100/1181, CE=5.1328, TRI=0.0336\n",
      "Epoch 1, batch 200/1181, CE=3.2683, TRI=0.0000\n",
      "Epoch 1, batch 300/1181, CE=2.4595, TRI=0.0010\n",
      "Epoch 1, batch 400/1181, CE=2.3437, TRI=0.0394\n",
      "Epoch 1, batch 500/1181, CE=1.8796, TRI=0.0000\n",
      "Epoch 1, batch 600/1181, CE=1.5260, TRI=0.0000\n",
      "Epoch 1, batch 700/1181, CE=1.4202, TRI=0.0000\n",
      "Epoch 1, batch 800/1181, CE=1.2068, TRI=0.0000\n",
      "Epoch 1, batch 900/1181, CE=0.7579, TRI=0.0000\n",
      "Epoch 1, batch 1000/1181, CE=0.7532, TRI=0.0018\n",
      "Epoch 1, batch 1100/1181, CE=0.7632, TRI=0.0224\n",
      "Epoch 1: CE=2.1562, TRI=0.0125\n"
     ]
    }
   ],
   "source": [
    "model = ReIDNet(\"resnet50\").to(DEVICE)\n",
    "model = ReIDNet(\"resnet18\").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Adam(model.parameters(), lr=3e-4, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "EPOCHS = 1  \n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    ce_loss, tri_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer,\n",
    "        epoch=epoch,\n",
    "        use_triplet=True,\n",
    "        alpha=1.0,\n",
    "        beta=1.0\n",
    "    )\n",
    "    print(f\"Epoch {epoch}: CE={ce_loss:.4f}, TRI={tri_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20541f7",
   "metadata": {},
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f2430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:10<00:00,  5.22it/s]\n",
      "100%|██████████| 362/362 [00:58<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:   45.64%\n",
      "Rank-1: 80.21%\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract features\n",
    "q_feats, q_pids, q_camids = extract_features(model, query_loader)\n",
    "g_feats, g_pids, g_camids = extract_features(model, gallery_loader)\n",
    "\n",
    "# 2. Compute distance matrix\n",
    "distmat = compute_distance_matrix(q_feats, g_feats)\n",
    "\n",
    "# 3. Evaluate\n",
    "mAP, rank1 = evaluate(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "print(f\"mAP:   {mAP*100:.2f}%\")\n",
    "print(f\"Rank-1: {rank1*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
